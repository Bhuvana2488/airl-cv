{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeZA/J9m+fmA9gjd3Gceir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuvana2488/airl-cv/blob/main/q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "print(\"🚀 Installing SAM 2 and dependencies...\")\n",
        "\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "!pip install -q supervision groundingdino-py\n",
        "!pip install -q opencv-python matplotlib torch torchvision pillow numpy\n",
        "\n",
        "print(\"✅ Installation complete!\")"
      ],
      "metadata": {
        "id": "UKRPuEbYCc3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0275f1-147d-483f-e62f-4cfab0800b1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Installing SAM 2 and dependencies...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "✅ Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🎮 Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLDPeQYVUjEq",
        "outputId": "5f2ca6bd-98c3-4dda-a9b0-4c5704478aea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎮 Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "checkpoint_dir = Path(\"checkpoints\")\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "model_cfg = \"sam2_hiera_small.yaml\"\n",
        "checkpoint_path = checkpoint_dir / \"sam2_hiera_small.pt\"\n",
        "\n",
        "if not checkpoint_path.exists():\n",
        "    print(\"📥 Downloading SAM 2 model checkpoint...\")\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P checkpoints/\n",
        "    print(\"✅ Download complete!\")\n",
        "else:\n",
        "    print(\"✅ Checkpoint already exists!\")\n",
        "\n",
        "print(f\"Model config: {model_cfg}\")\n",
        "print(f\"Checkpoint: {checkpoint_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ig3g1RLUmvq",
        "outputId": "8361e6cc-3c9b-4c68-994c-685a8d3bdef6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Checkpoint already exists!\n",
            "Model config: sam2_hiera_small.yaml\n",
            "Checkpoint: checkpoints/sam2_hiera_small.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Initialize SAM 2 Model (CORRECTED)\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "print(\"🔧 Initializing SAM 2...\")\n",
        "\n",
        "# Use the correct config path\n",
        "model_cfg = \"sam2_hiera_s.yaml\"  # Changed from sam2_hiera_small.yaml\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, str(checkpoint_path), device=device)\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "print(\"✅ SAM 2 model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2CjnkDEWWIo",
        "outputId": "dd0a79c3-c7f2-492e-b3fd-a98b5695396e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Initializing SAM 2...\n",
            "✅ SAM 2 model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Initialize GroundingDINO (CORRECTED)\n",
        "from groundingdino.util.inference import load_model, predict\n",
        "import os\n",
        "\n",
        "print(\"🔧 Initializing GroundingDINO...\")\n",
        "\n",
        "# Download config and weights\n",
        "os.makedirs(\"gdino_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Download GroundingDINO config\n",
        "if not os.path.exists(\"gdino_checkpoints/GroundingDINO_SwinT_OGC.py\"):\n",
        "    !wget -q https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py -P gdino_checkpoints/\n",
        "\n",
        "# Download GroundingDINO weights\n",
        "if not os.path.exists(\"gdino_checkpoints/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P gdino_checkpoints/\n",
        "\n",
        "# Load model with correct paths\n",
        "grounding_model = load_model(\n",
        "    model_config_path=\"gdino_checkpoints/GroundingDINO_SwinT_OGC.py\",\n",
        "    model_checkpoint_path=\"gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
        ")\n",
        "\n",
        "print(\"✅ GroundingDINO loaded successfully!\")\n",
        "\n",
        "BOX_THRESHOLD = 0.25\n",
        "TEXT_THRESHOLD = 0.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pPqrD47WmSX",
        "outputId": "7dea68f7-1a88-4941-c793-fa55f4b059f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Initializing GroundingDINO...\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GroundingDINO loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    if image_path.startswith('http'):\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(image_path, 'temp_image.jpg')\n",
        "        image_path = 'temp_image.jpg'\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def text_to_boxes(image, text_prompt, model):\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        caption=text_prompt,\n",
        "        box_threshold=BOX_THRESHOLD,\n",
        "        text_threshold=TEXT_THRESHOLD\n",
        "    )\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    boxes = boxes * torch.tensor([w, h, w, h])\n",
        "\n",
        "    boxes_xyxy = torch.zeros_like(boxes)\n",
        "    boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2\n",
        "    boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2\n",
        "    boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2\n",
        "    boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2\n",
        "\n",
        "    return boxes_xyxy.cpu().numpy(), logits.cpu().numpy(), phrases\n",
        "\n",
        "def visualize_results(image, masks, boxes, labels, title=\"Segmentation Results\"):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    img_boxes = image.copy()\n",
        "    for box, label in zip(boxes, labels):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_boxes, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(img_boxes, label, (x1, y1-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    axes[1].imshow(img_boxes)\n",
        "    axes[1].set_title(\"Detected Objects\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    img_masked = image.copy()\n",
        "    if len(masks) > 0:\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(masks)))[:, :3] * 255\n",
        "        for i, mask in enumerate(masks):\n",
        "            color = colors[i % len(colors)]\n",
        "            mask_bool = mask.astype(bool)\n",
        "            img_masked[mask_bool] = img_masked[mask_bool] * 0.5 + color * 0.5\n",
        "\n",
        "            contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                           cv2.RETR_EXTERNAL,\n",
        "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(img_masked, contours, -1, color.tolist(), 3)\n",
        "\n",
        "    axes[2].imshow(img_masked.astype(np.uint8))\n",
        "    axes[2].set_title(\"Final Segmentation\", fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ Helper functions defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN-uhr0AVMFy",
        "outputId": "17daa7cf-ad96-4f9e-ba66-e7043471730a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Helper functions defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main Text-Driven Segmentation Pipeline\n",
        "def text_driven_segmentation(image_path, text_prompt):\n",
        "    \"\"\"Complete pipeline: text prompt → object detection → segmentation\"\"\"\n",
        "    print(f\"🎯 Processing: '{text_prompt}'\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Step 1: Load image\n",
        "    print(\"📸 Step 1/4: Loading image...\")\n",
        "    image = load_image(image_path)\n",
        "    print(f\"   Image shape: {image.shape}\")\n",
        "\n",
        "    # Step 2: Text-to-box detection\n",
        "    print(f\"🔍 Step 2/4: Detecting '{text_prompt}' in image...\")\n",
        "    boxes, confidences, labels = text_to_boxes(image, text_prompt, grounding_model)\n",
        "    print(f\"   Found {len(boxes)} object(s)\")\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        print(\"   ⚠️  No objects detected! Try a different prompt or lower threshold.\")\n",
        "        return None, None, None\n",
        "\n",
        "    for i, (label, conf) in enumerate(zip(labels, confidences)):\n",
        "        print(f\"   - Object {i+1}: {label} (confidence: {conf:.2f})\")\n",
        "\n",
        "    # Step 3: Set image for SAM 2\n",
        "    print(\"🎨 Step 3/4: Preparing SAM 2...\")\n",
        "    sam2_predictor.set_image(image)\n",
        "\n",
        "    # Step 4: Generate masks\n",
        "    print(\"✨ Step 4/4: Generating segmentation masks...\")\n",
        "    masks_list = []\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        masks, scores, _ = sam2_predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=box,\n",
        "            multimask_output=False\n",
        "        )\n",
        "        masks_list.append(masks[0])\n",
        "\n",
        "    masks_array = np.array(masks_list)\n",
        "    print(f\"   Generated {len(masks_array)} mask(s)\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"✅ Segmentation complete!\")\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(image, masks_array, boxes, labels,\n",
        "                         title=f\"Text-Driven Segmentation: '{text_prompt}'\")\n",
        "\n",
        "    return masks_array, boxes, labels\n",
        "\n",
        "print(\"✅ Pipeline function ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y87Vii9NVZFS",
        "outputId": "1b1b6259-9336-4bd8-9ffe-bb5dc9df6a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Pipeline function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions (FIXED)\n",
        "import torch\n",
        "from PIL import Image as PILImage\n",
        "from groundingdino.util.inference import Model, predict # Import predict here\n",
        "import groundingdino.datasets.transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"Load image from file or URL\"\"\"\n",
        "    if image_path.startswith('http'):\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(image_path, 'temp_image.jpg')\n",
        "        image_path = 'temp_image.jpg'\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def text_to_boxes(image, text_prompt, model):\n",
        "    \"\"\"Convert text prompt to bounding boxes using GroundingDINO\"\"\"\n",
        "    # Save image temporarily\n",
        "    temp_path = \"temp_for_dino.jpg\"\n",
        "    cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Load as PIL Image\n",
        "    image_pil = PILImage.open(temp_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply transforms\n",
        "    transform = T.Compose([\n",
        "        T.RandomResize([800], max_size=1333),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    image_transformed, _ = transform(image_pil, None)\n",
        "\n",
        "    # Predict\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=model,\n",
        "        image=image_transformed,\n",
        "        caption=text_prompt,\n",
        "        box_threshold=BOX_THRESHOLD,\n",
        "        text_threshold=TEXT_THRESHOLD,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Convert to absolute coordinates\n",
        "    h, w = image.shape[:2]\n",
        "    boxes = boxes * torch.tensor([w, h, w, h]).to(boxes.device)\n",
        "\n",
        "    # Convert from cxcywh to xyxy format\n",
        "    boxes_xyxy = torch.zeros_like(boxes)\n",
        "    boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1\n",
        "    boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1\n",
        "    boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2  # x2\n",
        "    boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2  # y2\n",
        "\n",
        "    return boxes_xyxy.cpu().numpy(), logits.cpu().numpy(), phrases\n",
        "\n",
        "def visualize_results(image, masks, boxes, labels, title=\"Segmentation Results\"):\n",
        "    \"\"\"Visualize segmentation results with masks and boxes\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Image with bounding boxes\n",
        "    img_boxes = image.copy()\n",
        "    for box, label in zip(boxes, labels):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_boxes, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        cv2.putText(img_boxes, label, (x1, y1-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    axes[1].imshow(img_boxes)\n",
        "    axes[1].set_title(\"Detected Objects\", fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Segmentation masks overlay\n",
        "    img_masked = image.copy()\n",
        "    if len(masks) > 0:\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(masks)))[:, :3] * 255\n",
        "        for i, mask in enumerate(masks):\n",
        "            color = colors[i % len(colors)]\n",
        "            mask_bool = mask.astype(bool)\n",
        "            img_masked[mask_bool] = img_masked[mask_bool] * 0.5 + color * 0.5\n",
        "\n",
        "            # Draw contours\n",
        "            contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                           cv2.RETR_EXTERNAL,\n",
        "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(img_masked, contours, -1, color.tolist(), 3)\n",
        "\n",
        "    axes[2].imshow(img_masked.astype(np.uint8))\n",
        "    axes[2].set_title(\"Final Segmentation\", fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return img_masked\n",
        "\n",
        "print(\"✅ Helper functions defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sviF2q6oXprX",
        "outputId": "acdb057e-16e3-4d87-f488-626b33bacabc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Helper functions defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced - Multiple Object Segmentation\n",
        "def multi_object_segmentation(image_path, text_prompts):\n",
        "    \"\"\"Segment multiple different objects using different prompts\"\"\"\n",
        "    print(f\"🎯 Multi-Object Segmentation\")\n",
        "    print(f\"   Prompts: {', '.join(text_prompts)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    image = load_image(image_path)\n",
        "\n",
        "    all_masks = []\n",
        "    all_boxes = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Set image for SAM once\n",
        "    sam2_predictor.set_image(image)\n",
        "\n",
        "    for prompt in text_prompts:\n",
        "        print(f\"\\n🔍 Processing '{prompt}'...\")\n",
        "\n",
        "        boxes, confidences, labels = text_to_boxes(image, prompt, grounding_model)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            print(f\"   ⚠️  No '{prompt}' detected\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Found {len(boxes)} instance(s)\")\n",
        "\n",
        "        for box in boxes:\n",
        "            masks, scores, _ = sam2_predictor.predict(\n",
        "                point_coords=None,\n",
        "                point_labels=None,\n",
        "                box=box,\n",
        "                multimask_output=False\n",
        "            )\n",
        "            all_masks.append(masks[0])\n",
        "            all_boxes.append(box)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "\n",
        "    if len(all_masks) == 0:\n",
        "        print(\"\\n❌ No objects detected with any prompt!\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"\\n✅ Total objects segmented: {len(all_masks)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    masks_array = np.array(all_masks)\n",
        "    boxes_array = np.array(all_boxes)\n",
        "    visualize_results(image, masks_array, boxes_array, all_labels,\n",
        "                         title=f\"Multi-Object: {', '.join(text_prompts)}\")\n",
        "\n",
        "    return masks_array, boxes_array, all_labels\n",
        "\n",
        "print(\"✅ Multi-object function ready!\")\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n💡 Try it:\")\n",
        "print(\"   image_url = 'https://images.unsplash.com/photo-1449965408869-eaa3f722e40d?w=800'\")\n",
        "print(\"   prompts = ['car', 'person']\")\n",
        "print(\"   multi_object_segmentation(image_url, prompts)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzQj06qdYBw-",
        "outputId": "93c6de88-e28a-428b-b5d9-74cbf04522dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Multi-object function ready!\n",
            "\n",
            "💡 Try it:\n",
            "   image_url = 'https://images.unsplash.com/photo-1449965408869-eaa3f722e40d?w=800'\n",
            "   prompts = ['car', 'person']\n",
            "   multi_object_segmentation(image_url, prompts)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export & Save Results\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "def save_segmentation_results(image, masks, boxes, labels, output_name=\"result\"):\n",
        "    \"\"\"Save segmentation results to files\"\"\"\n",
        "    print(f\"💾 Saving results as '{output_name}'...\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = f\"outputs_{output_name}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save overlay image\n",
        "    img_overlay = image.copy()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(masks)))[:, :3] * 255\n",
        "\n",
        "    for i, mask in enumerate(masks):\n",
        "        color = colors[i % len(colors)]\n",
        "        mask_bool = mask.astype(bool)\n",
        "        img_overlay[mask_bool] = img_overlay[mask_bool] * 0.5 + color * 0.5\n",
        "\n",
        "        contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                       cv2.RETR_EXTERNAL,\n",
        "                                       cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(img_overlay, contours, -1, color.tolist(), 3)\n",
        "\n",
        "    overlay_path = os.path.join(output_dir, f\"{output_name}_overlay.png\")\n",
        "    cv2.imwrite(overlay_path, cv2.cvtColor(img_overlay.astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "    print(f\"   ✅ Saved overlay: {overlay_path}\")\n",
        "\n",
        "    # Save individual masks\n",
        "    for i, mask in enumerate(masks):\n",
        "        mask_path = os.path.join(output_dir, f\"{output_name}_mask_{i+1}.png\")\n",
        "        cv2.imwrite(mask_path, (mask * 255).astype(np.uint8))\n",
        "    print(f\"   ✅ Saved {len(masks)} individual mask(s)\")\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"image_shape\": image.shape,\n",
        "        \"num_objects\": len(masks),\n",
        "        \"labels\": labels,\n",
        "        \"boxes\": boxes.tolist() if isinstance(boxes, np.ndarray) else boxes,\n",
        "        \"parameters\": {\n",
        "            \"box_threshold\": BOX_THRESHOLD,\n",
        "            \"text_threshold\": TEXT_THRESHOLD,\n",
        "            \"model\": \"SAM2-Hiera-Small\",\n",
        "            \"detector\": \"GroundingDINO\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(output_dir, f\"{output_name}_metadata.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"   ✅ Saved metadata: {metadata_path}\")\n",
        "\n",
        "    # Create zip file\n",
        "    zip_path = f\"{output_name}_results\"\n",
        "    shutil.make_archive(zip_path, 'zip', output_dir)\n",
        "    print(f\"\\n📦 Created zip file: {zip_path}.zip\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "print(\"✅ Save function ready!\")\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n💡 After running segmentation, save with:\")\n",
        "print(\"   save_segmentation_results(image, masks, boxes, labels, 'my_segmentation')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NacK8TRzYL6W",
        "outputId": "30aeff5f-dd35-4153-a9e9-432c370d18b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Save function ready!\n",
            "\n",
            "💡 After running segmentation, save with:\n",
            "   save_segmentation_results(image, masks, boxes, labels, 'my_segmentation')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmentation Quality Analysis\n",
        "def analyze_segmentation_quality(masks, boxes, image_shape):\n",
        "    \"\"\"Analyze segmentation quality metrics\"\"\"\n",
        "    print(\"\\n📊 Segmentation Quality Analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    h, w = image_shape[:2]\n",
        "    total_pixels = h * w\n",
        "\n",
        "    for i, (mask, box) in enumerate(zip(masks, boxes)):\n",
        "        print(f\"\\n🔍 Object {i+1}:\")\n",
        "\n",
        "        # Mask statistics\n",
        "        mask_area = np.sum(mask)\n",
        "        mask_percentage = (mask_area / total_pixels) * 100\n",
        "\n",
        "        # Box statistics\n",
        "        x1, y1, x2, y2 = box\n",
        "        box_area = (x2 - x1) * (y2 - y1)\n",
        "        box_percentage = (box_area / total_pixels) * 100\n",
        "\n",
        "        # Mask-to-box ratio\n",
        "        fill_ratio = (mask_area / box_area) * 100 if box_area > 0 else 0\n",
        "\n",
        "        print(f\"   Mask Area: {mask_area:,} pixels ({mask_percentage:.2f}% of image)\")\n",
        "        print(f\"   Box Area: {box_area:,.0f} pixels ({box_percentage:.2f}% of image)\")\n",
        "        print(f\"   Fill Ratio: {fill_ratio:.1f}% (mask/box)\")\n",
        "\n",
        "        # Shape compactness\n",
        "        contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                       cv2.RETR_EXTERNAL,\n",
        "                                       cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if contours:\n",
        "            perimeter = cv2.arcLength(contours[0], True)\n",
        "            compactness = (4 * np.pi * mask_area) / (perimeter ** 2) if perimeter > 0 else 0\n",
        "            print(f\"   Compactness: {compactness:.3f} (1.0 = perfect circle)\")\n",
        "\n",
        "            # Quality indicators\n",
        "            print(f\"   Quality Indicators:\")\n",
        "            if fill_ratio > 80:\n",
        "                print(f\"      ✅ High fill ratio - tight segmentation\")\n",
        "            elif fill_ratio > 50:\n",
        "                print(f\"      ⚠️  Moderate fill ratio\")\n",
        "            else:\n",
        "                print(f\"      ❌ Low fill ratio - may need refinement\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    print(\"✅ Analysis function ready!\")\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\n💡 After segmentation, analyze with:\")\n",
        "    print(\"   analyze_segmentation_quality(masks, boxes, image.shape)\")"
      ],
      "metadata": {
        "id": "IHYb3cydYi2a"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create Comparison Grid\n",
        "def create_comparison_grid(image_path, text_prompts, save=True):\n",
        "    \"\"\"Create a comparison grid of different segmentation prompts\"\"\"\n",
        "    print(f\"🎨 Creating comparison grid...\")\n",
        "\n",
        "    image = load_image(image_path)\n",
        "    n_prompts = len(text_prompts)\n",
        "\n",
        "    fig, axes = plt.subplots(1, n_prompts + 1, figsize=(6 * (n_prompts + 1), 6))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Set image for SAM\n",
        "    sam2_predictor.set_image(image)\n",
        "\n",
        "    # Process each prompt\n",
        "    for idx, prompt in enumerate(text_prompts):\n",
        "        print(f\"   Processing '{prompt}'...\")\n",
        "\n",
        "        boxes, confidences, labels = text_to_boxes(image, prompt, grounding_model)\n",
        "\n",
        "        img_result = image.copy()\n",
        "        if len(boxes) > 0:\n",
        "            for box in boxes:\n",
        "                masks, scores, _ = sam2_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=box,\n",
        "                    multimask_output=False\n",
        "                )\n",
        "\n",
        "                mask = masks[0]\n",
        "                color = np.array([255, 0, 0])\n",
        "                mask_bool = mask.astype(bool)\n",
        "                img_result[mask_bool] = img_result[mask_bool] * 0.5 + color * 0.5\n",
        "\n",
        "                contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                               cv2.RETR_EXTERNAL,\n",
        "                                               cv2.CHAIN_APPROX_SIMPLE)\n",
        "                cv2.drawContours(img_result, contours, -1, (255, 0, 0), 2)\n",
        "\n",
        "        axes[idx + 1].imshow(img_result.astype(np.uint8))\n",
        "        axes[idx + 1].set_title(f\"'{prompt}'\\n{len(boxes)} detected\",\n",
        "                               fontsize=12, fontweight='bold')\n",
        "        axes[idx + 1].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Text Prompt Comparison\", fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(\"comparison_grid.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"   ✅ Saved: comparison_grid.png\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ Comparison function ready!\")\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n💡 Try it:\")\n",
        "print(\"   image_url = 'https://images.unsplash.com/photo-1583511655857-d19b40a7a54e?w=800'\")\n",
        "print(\"   prompts = ['dog', 'animal', 'pet']\")\n",
        "print(\"   create_comparison_grid(image_url, prompts)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNF5nBwEYpXf",
        "outputId": "2df70de1-e76f-424e-ae2b-a2e3711037b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Comparison function ready!\n",
            "\n",
            "💡 Try it:\n",
            "   image_url = 'https://images.unsplash.com/photo-1583511655857-d19b40a7a54e?w=800'\n",
            "   prompts = ['dog', 'animal', 'pet']\n",
            "   create_comparison_grid(image_url, prompts)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Segmentation Demo\n",
        "def run_interactive_demo():\n",
        "    \"\"\"Run an interactive demo with multiple examples\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"  🎯 INTERACTIVE SEGMENTATION DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Example 1: Single object\n",
        "    print(\"\\n📌 EXAMPLE 1: Single Object Segmentation\")\n",
        "    print(\"-\"*70)\n",
        "    image_url = \"https://images.unsplash.com/photo-1583511655857-d19b40a7a54e?w=800\"\n",
        "    masks, boxes, labels = text_driven_segmentation(image_url, \"dog\")\n",
        "\n",
        "    if masks is not None:\n",
        "        analyze_segmentation_quality(masks, boxes, load_image(image_url).shape)\n",
        "\n",
        "    # Example 2: Multiple prompts\n",
        "    print(\"\\n📌 EXAMPLE 2: Multiple Objects\")\n",
        "    print(\"-\"*70)\n",
        "    street_url = \"https://images.unsplash.com/photo-1449965408869-eaa3f722e40d?w=800\"\n",
        "    multi_object_segmentation(street_url, ['car', 'person'])\n",
        "\n",
        "    # Example 3: Comparison\n",
        "    print(\"\\n📌 EXAMPLE 3: Prompt Comparison\")\n",
        "    print(\"-\"*70)\n",
        "    create_comparison_grid(image_url, ['dog', 'animal'], save=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  ✅ DEMO COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n💡 Now try with your own images and prompts!\")\n",
        "\n",
        "print(\"✅ Interactive demo ready!\")\n",
        "print(\"\\n▶️  Run: run_interactive_demo()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bbwlvJPYswW",
        "outputId": "0b358824-6529-4f68-869a-da78dd2a301d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Interactive demo ready!\n",
            "\n",
            "▶️  Run: run_interactive_demo()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Threshold Testing\n",
        "def test_thresholds(image_path, text_prompt, thresholds=[0.20, 0.25, 0.30]):\n",
        "    \"\"\"Test different detection thresholds\"\"\"\n",
        "    print(f\"🔬 Threshold Sensitivity Analysis: '{text_prompt}'\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    image = load_image(image_path)\n",
        "    global BOX_THRESHOLD\n",
        "\n",
        "    original_box_threshold = BOX_THRESHOLD # Store original threshold\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        BOX_THRESHOLD = thresh\n",
        "\n",
        "        boxes, confidences, labels = text_to_boxes(image, text_prompt, grounding_model)\n",
        "        print(f\"   Threshold {thresh:.2f}: {len(boxes)} detections\")\n",
        "\n",
        "        if len(boxes) > 0:\n",
        "            avg_conf = np.mean(confidences)\n",
        "            print(f\"      Average confidence: {avg_conf:.3f}\")\n",
        "\n",
        "    # Reset to original threshold\n",
        "    BOX_THRESHOLD = original_box_threshold\n",
        "\n",
        "    print(\"\\n💡 Recommendation:\")\n",
        "    print(\"   - Lower threshold (0.20): More detections\")\n",
        "    print(\"   - Default (0.25): Balanced\")\n",
        "    print(\"   - Higher threshold (0.30): Fewer but more confident\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"✅ Threshold testing ready!\")\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n💡 Test with:\")\n",
        "print(\"   test_thresholds('image_url', 'dog', [0.20, 0.25, 0.30])\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKmwkrb-Yz7H",
        "outputId": "06d2a068-a40e-48db-db40-cb4a3ef9ad06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Threshold testing ready!\n",
            "\n",
            "💡 Test with:\n",
            "   test_thresholds('image_url', 'dog', [0.20, 0.25, 0.30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BONUS - Upload and Process Video\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time # Import time for potential delay\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"  🎬 BONUS: TEXT-DRIVEN VIDEO SEGMENTATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n📋 Instructions:\")\n",
        "print(\"   1. Upload a video file (10-30 seconds recommended)\")\n",
        "print(\"   2. Enter the object you want to track\")\n",
        "print(\"   3. Wait for processing (may take 30-60 seconds)\")\n",
        "print(\"   4. Watch the segmented video!\\n\")\n",
        "print(\"💡 Tips:\")\n",
        "print(\"   - Use clear, stable videos for best results\")\n",
        "print(\"   - Object should be visible in the first frame\")\n",
        "print(\"   - Shorter videos = faster processing\")\n",
        "print(\"   - Try: 'person', 'car', 'dog', 'ball', etc.\\n\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Upload video\n",
        "print(\"📤 Upload your video file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    print(f\"\\n✅ Video uploaded: {video_path}\")\n",
        "\n",
        "    # Get video info\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"❌ Error: Could not open video file {video_path}\")\n",
        "    else:\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = total_frames / fps if fps > 0 else 0\n",
        "        cap.release()\n",
        "\n",
        "        print(f\"📊 Video Info:\")\n",
        "        print(f\"   - Total frames: {total_frames}\")\n",
        "        print(f\"   - FPS: {fps:.1f}\")\n",
        "        print(f\"   - Duration: {duration:.1f} seconds\")\n",
        "\n",
        "        # Get text prompt\n",
        "        text_prompt = input(\"\\n🎯 Enter object to track (e.g., 'person', 'car', 'dog'), then PRESS ENTER: \").strip()\n",
        "\n",
        "        if not text_prompt:\n",
        "            print(\"❌ No prompt entered!\")\n",
        "        else:\n",
        "            # Process video (limit to 90 frames for Colab T4)\n",
        "            max_frames = min(90, total_frames)\n",
        "\n",
        "            if total_frames > 90:\n",
        "                print(f\"\\n⚠️  Video has {total_frames} frames. Processing first {max_frames} frames.\")\n",
        "                print(f\"   This is ~{max_frames/fps:.1f} seconds of video.\")\n",
        "\n",
        "            print(f\"\\n⏳ Processing '{text_prompt}'... This may take 30-60 seconds...\\n\")\n",
        "\n",
        "            # Assuming text_driven_video_segmentation is defined elsewhere and takes video_path, text_prompt, and max_frames\n",
        "            # Need to implement or use the actual video processing function here.\n",
        "            # Placeholder for the actual video processing call:\n",
        "            # output = text_driven_video_segmentation(video_path, text_prompt, max_frames=max_frames)\n",
        "\n",
        "            # Since the video processing function is not provided, I'll add a placeholder message.\n",
        "            print(\"🚧 Video processing function (text_driven_video_segmentation) is not defined in the current notebook state.\")\n",
        "            print(\"   Please ensure this function is defined and runnable.\")\n",
        "            output = None # Set output to None as processing didn't happen\n",
        "\n",
        "            if output:\n",
        "                print(f\"\\n🎉 SUCCESS! Video segmentation complete!\")\n",
        "                print(f\"📁 Output saved as: {output}\")\n",
        "                print(f\"\\n💾 Download the video from the file browser (left sidebar)\")\n",
        "            else:\n",
        "                print(f\"\\n❌ Video segmentation failed or function not found. Try a different prompt or video.\")\n",
        "else:\n",
        "    print(\"❌ No video uploaded!\")\n",
        "    print(\"\\n💡 Alternative: Use sample video URL\")\n",
        "    print(\"   You can also download a video first:\")\n",
        "    print(\"   !wget https://example.com/sample_video.mp4 -O sample.mp4\")\n",
        "    print(\"   Then run: text_driven_video_segmentation('sample.mp4', 'person', 60)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "3Y8cV15UZFrw",
        "outputId": "30af939a-6427-429c-925b-d9d9642fb26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "  🎬 BONUS: TEXT-DRIVEN VIDEO SEGMENTATION\n",
            "======================================================================\n",
            "\n",
            "📋 Instructions:\n",
            "   1. Upload a video file (10-30 seconds recommended)\n",
            "   2. Enter the object you want to track\n",
            "   3. Wait for processing (may take 30-60 seconds)\n",
            "   4. Watch the segmented video!\n",
            "\n",
            "💡 Tips:\n",
            "   - Use clear, stable videos for best results\n",
            "   - Object should be visible in the first frame\n",
            "   - Shorter videos = faster processing\n",
            "   - Try: 'person', 'car', 'dog', 'ball', etc.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "📤 Upload your video file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6d4c39f-900f-4608-86eb-028fdbd1829b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f6d4c39f-900f-4608-86eb-028fdbd1829b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 8833602-uhd_4096_2160_25fps.mp4 to 8833602-uhd_4096_2160_25fps.mp4\n",
            "\n",
            "✅ Video uploaded: 8833602-uhd_4096_2160_25fps.mp4\n",
            "📊 Video Info:\n",
            "   - Total frames: 398\n",
            "   - FPS: 25.0\n",
            "   - Duration: 15.9 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: BONUS - Fast Video Processing (OPTIMIZED)\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time # Import time for potential delay\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"  🎬 BONUS: TEXT-DRIVEN VIDEO SEGMENTATION (FAST MODE)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n📋 Instructions:\")\n",
        "print(\"   1. Upload a SHORT video (5-10 seconds recommended)\")\n",
        "print(\"   2. Enter the object you want to track\")\n",
        "print(\"   3. Processing time: ~15-30 seconds\")\n",
        "print(\"\\n💡 Tips for FAST processing:\")\n",
        "print(\"   - Use videos under 10 seconds\")\n",
        "print(\"   - Lower resolution = faster processing\")\n",
        "print(\"   - We'll process every 2nd frame for speed\\n\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Upload video\n",
        "print(\"📤 Upload your video file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    print(f\"\\n✅ Video uploaded: {video_path}\")\n",
        "\n",
        "    # Get video info\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"❌ Error: Could not open video file {video_path}\")\n",
        "    else:\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = total_frames / fps if fps > 0 else 0\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.release()\n",
        "\n",
        "        print(f\"📊 Video Info:\")\n",
        "        print(f\"   - Duration: {duration:.1f}s\")\n",
        "        print(f\"   - Total frames: {total_frames}\")\n",
        "        print(f\"   - Resolution: {width}x{height}\")\n",
        "        print(f\"   - FPS: {fps:.1f}\")\n",
        "\n",
        "        # Auto-calculate optimal frame limit for fast processing\n",
        "        if duration > 10:\n",
        "            max_frames = 60  # ~2 seconds\n",
        "            print(f\"\\n⚠️  Long video detected! Processing first 2 seconds only.\")\n",
        "        elif duration > 5:\n",
        "            max_frames = 90  # ~3 seconds\n",
        "            print(f\"\\n✅ Processing first 3 seconds\")\n",
        "        else:\n",
        "            max_frames = min(total_frames, 150)  # All frames if short\n",
        "            print(f\"\\n✅ Processing entire video\")\n",
        "\n",
        "        print(f\"   Will process: {max_frames} frames (~{max_frames/fps:.1f}s)\")\n",
        "\n",
        "        # Get text prompt\n",
        "        text_prompt = input(\"\\n🎯 Enter object to track, then PRESS ENTER: \").strip()\n",
        "\n",
        "        if not text_prompt:\n",
        "            print(\"❌ No prompt entered!\")\n",
        "        else:\n",
        "            print(f\"\\n⏳ Processing '{text_prompt}'... Please wait 15-30 seconds...\\n\")\n",
        "\n",
        "            # Process video with optimized settings\n",
        "            # Assuming text_driven_video_segmentation and create_video_summary are defined elsewhere\n",
        "            # Placeholder for the actual video processing and summary calls:\n",
        "            # output = text_driven_video_segmentation(video_path, text_prompt, max_frames=max_frames)\n",
        "            # if output:\n",
        "            #     create_video_summary(video_path, output)\n",
        "\n",
        "            # Since the video processing and summary functions are not provided, I'll add placeholder messages.\n",
        "            print(\"🚧 Video processing function (text_driven_video_segmentation) is not defined in the current notebook state.\")\n",
        "            print(\"   Please ensure this function is defined and runnable.\")\n",
        "            output = None # Set output to None as processing didn't happen\n",
        "            create_video_summary_placeholder = True # Flag to indicate that summary would be called\n",
        "\n",
        "            if output:\n",
        "                print(f\"\\n🎉 SUCCESS! Video segmentation complete!\")\n",
        "                print(f\"📁 Output: {output}\")\n",
        "\n",
        "                # Create comparison\n",
        "                if create_video_summary_placeholder:\n",
        "                     print(f\"\\n📊 Creating before/after comparison...\")\n",
        "                     print(\"🚧 Video summary function (create_video_summary) is not defined in the current notebook state.\")\n",
        "                     print(\"   Please ensure this function is defined and runnable.\")\n",
        "\n",
        "                print(f\"\\n💾 To download:\")\n",
        "                print(f\"   - Click folder icon (left sidebar)\")\n",
        "                print(f\"   - Right-click '{output}' → Download\")\n",
        "            else:\n",
        "                print(f\"\\n❌ Failed. Try:\")\n",
        "                print(f\"   - Different prompt (e.g., 'person', 'car', 'dog')\")\n",
        "                print(f\"   - Shorter video\")\n",
        "                print(f\"   - Object must be visible in first frame\")\n",
        "else:\n",
        "    print(\"❌ No video uploaded!\")"
      ],
      "metadata": {
        "id": "JK8VViTZP0ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: BONUS - Video Segmentation Summary\n",
        "\n",
        "def create_video_summary(video_path, output_path):\n",
        "    \"\"\"Create a summary comparison showing original vs segmented\"\"\"\n",
        "    if not os.path.exists(video_path) or not os.path.exists(output_path):\n",
        "        print(\"❌ Video files not found!\")\n",
        "        return\n",
        "\n",
        "    print(\"📊 Creating video comparison summary...\")\n",
        "\n",
        "    # Read first, middle, and last frames from both videos\n",
        "    cap_orig = cv2.VideoCapture(video_path)\n",
        "    cap_seg = cv2.VideoCapture(output_path)\n",
        "\n",
        "    total_frames = int(cap_orig.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = [0, total_frames//2, total_frames-1]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(\"Video Segmentation: Before & After\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    titles = ['First Frame', 'Middle Frame', 'Last Frame']\n",
        "\n",
        "    for idx, frame_idx in enumerate(frame_indices):\n",
        "        # Original\n",
        "        cap_orig.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame_orig = cap_orig.read()\n",
        "        if ret:\n",
        "            axes[0, idx].imshow(cv2.cvtColor(frame_orig, cv2.COLOR_BGR2RGB))\n",
        "            axes[0, idx].set_title(f\"Original - {titles[idx]}\", fontweight='bold')\n",
        "            axes[0, idx].axis('off')\n",
        "\n",
        "        # Segmented\n",
        "        cap_seg.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame_seg = cap_seg.read()\n",
        "        if ret:\n",
        "            axes[1, idx].imshow(cv2.cvtColor(frame_seg, cv2.COLOR_BGR2RGB))\n",
        "            axes[1, idx].set_title(f\"Segmented - {titles[idx]}\", fontweight='bold')\n",
        "            axes[1, idx].axis('off')\n",
        "\n",
        "    cap_orig.release()\n",
        "    cap_seg.release()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"video_summary.png\", dpi=150, bbox_inches='tight')\n",
        "    print(\"✅ Summary saved as: video_summary.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✅ Video summary function ready!\")\n",
        "    print(\"\\n💡 After video segmentation, create summary with:\")\n",
        "    print(\"   create_video_summary('input_video.mp4', 'segmented_video.mp4')\")"
      ],
      "metadata": {
        "id": "3v6cNiKxJ3G_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}